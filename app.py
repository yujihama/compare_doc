import streamlit as st
import os
import re
from doc_compare.pdf_util import extract_chunks_by_headings
from doc_compare.text_processing import split_sentences, get_embeddings, process_chunks_to_chunk_embeddings
from doc_compare.main_processor import process_document_comparison
from doc_compare.ui_components import render_comparison_report, filter_report_by_change_types
from doc_compare.cache_util import save_embeddings_cache, load_embeddings_cache, clear_cache, get_cache_info, calculate_text_hash
from doc_compare.markdown_exporter import export_all_formats, export_to_markdown, export_summary_to_markdown, export_to_json, export_to_csv
from doc_compare.config import SIMILARITY_THRESHOLDS
import logging
import numpy as np
import json
from datetime import datetime
from doc_compare.config import CACHE_CONFIG
import traceback

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)

# OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
openai_api_key = os.getenv("OPENAI_API_KEY")

def write_groups_debug(groups):
    """ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±ã‚’ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã«æ›¸ãè¾¼ã¿"""
    lines = [f"Total groups: {len(groups)}"]
    for i, group in enumerate(groups, 1):
        lines.append(f"Group {i}:")
        lines.append(f"  old: {', '.join(group['old'])}")
        lines.append(f"  new: {', '.join(group['new'])}")
    with open("group_debug.log", "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

def get_or_create_embeddings(chunks, prefix):
    """ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å–å¾—ã¾ãŸã¯æ–°è¦ä½œæˆ"""
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã‚’ç¢ºèª
    cache_info = get_cache_info(prefix)
    if cache_info["exists"]:
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
            from doc_compare.config import CACHE_CONFIG
            cache_dir = CACHE_CONFIG["cache_directory"]
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®æ§‹ç¯‰
            def get_cache_path(filename):
                return os.path.join(cache_dir, filename)
            
            # æ–‡åˆ†å‰²çµæœèª­ã¿è¾¼ã¿
            sentences_file = get_cache_path(f"{prefix}_sentences.json")
            with open(sentences_file, "r", encoding="utf-8") as f:
                sent_lists = json.load(f)
            
            # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°èª­ã¿è¾¼ã¿
            embeddings_file = get_cache_path(f"{prefix}_embeddings.npz")
            cache_data = np.load(embeddings_file)
            embeddings_array = cache_data["embeddings"]
            indices = cache_data["indices"]
            
            # ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å…ƒã®æ§‹é€ ã«å¾©å…ƒ
            embeddings = [[] for _ in chunks]
            for i, (chunk_idx, sent_idx) in enumerate(indices):
                if chunk_idx < len(embeddings):
                    embeddings[chunk_idx].append(embeddings_array[i].tolist())
            
            return sent_lists, embeddings
            
        except Exception as e:
            logging.warning(f"{prefix}ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ãŸå ´åˆã¯æ–°è¦ä½œæˆ
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„å ´åˆã¯æ–°è¦ä½œæˆ
    logging.info(f"{prefix}ã®æ–‡åˆ†å‰²ã¨ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°é–‹å§‹")
    sent_lists = [split_sentences(c["text"]) for c in chunks]
    embeddings = [get_embeddings(sents) if sents else [] for sents in sent_lists]
    
    save_embeddings_cache(chunks, sent_lists, embeddings, prefix)
    return sent_lists, embeddings

def get_or_create_chunk_embeddings(chunks, prefix):
    """ãƒãƒ£ãƒ³ã‚¯å…¨ä½“ã®ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’å–å¾—ã¾ãŸã¯æ–°è¦ä½œæˆ (ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œãƒ»å­˜åœ¨ã™ã‚Œã°å¸¸ã«ä½¿ç”¨)"""
    if not chunks:
        logging.info(f"{prefix}: ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã®ãŸã‚ã€ãƒãƒ£ãƒ³ã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
        return []

    cache_dir = CACHE_CONFIG.get("cache_directory", ".cache")
    os.makedirs(cache_dir, exist_ok=True)
    cache_file = os.path.join(cache_dir, f"{prefix}_chunk_embeddings.npz")

    try:
        if os.path.exists(cache_file):
            logging.info(f"'{prefix}' ã®ãƒãƒ£ãƒ³ã‚¯å…¨ä½“ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚å†…å®¹ã®æ¤œè¨¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ä½¿ç”¨ã—ã¾ã™: {cache_file}")
            cache_data = np.load(cache_file, allow_pickle=True)
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã« 'chunk_embeddings' ã‚­ãƒ¼ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã ã‘ã‚’æœŸå¾…ã™ã‚‹
            if 'chunk_embeddings' in cache_data:
                return cache_data['chunk_embeddings'].tolist() if isinstance(cache_data['chunk_embeddings'], np.ndarray) else cache_data['chunk_embeddings']
            else:
                logging.warning(f"'{prefix}' ã®ãƒãƒ£ãƒ³ã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã« 'chunk_embeddings' ã‚­ãƒ¼ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚å†ä½œæˆã—ã¾ã™: {cache_file}")
        
    except Exception as e:
        logging.warning(f"'{prefix}' ã®ãƒãƒ£ãƒ³ã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿/æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}ã€‚å†ä½œæˆã—ã¾ã™ã€‚")

    logging.info(f"'{prefix}' ã®ãƒãƒ£ãƒ³ã‚¯å…¨ä½“ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ–°è¦ä½œæˆã—ã¾ã™ã€‚")
    chunk_embeddings_data = process_chunks_to_chunk_embeddings(chunks)
    
    if not isinstance(chunk_embeddings_data, np.ndarray):
        if chunk_embeddings_data is None or (isinstance(chunk_embeddings_data, list) and not chunk_embeddings_data):
            logging.warning(f"'{prefix}' ã® process_chunks_to_chunk_embeddings ã®çµæœãŒç©ºã¾ãŸã¯Noneã§ã™ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä½œæˆã•ã‚Œã¾ã›ã‚“ã€‚")
            return []
        try:
            chunk_embeddings_array = np.array(chunk_embeddings_data)
        except Exception as e:
            logging.error(f"'{prefix}' ã®ãƒãƒ£ãƒ³ã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’NumPyé…åˆ—ã«å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä½œæˆã•ã‚Œã¾ã›ã‚“ã€‚")
            return chunk_embeddings_data
    else:
        chunk_embeddings_array = chunk_embeddings_data
    
    if chunk_embeddings_array.size == 0:
        logging.warning(f"'{prefix}' ã®ãƒãƒ£ãƒ³ã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°é…åˆ—ãŒç©ºã§ã™ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯ä½œæˆã•ã‚Œã¾ã›ã‚“ã€‚")
        return chunk_embeddings_array.tolist() if isinstance(chunk_embeddings_array, np.ndarray) else chunk_embeddings_array

    try:
        np.savez_compressed(cache_file, chunk_embeddings=chunk_embeddings_array)
        logging.info(f"'{prefix}' ã®ãƒãƒ£ãƒ³ã‚¯å…¨ä½“ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜ã—ã¾ã—ãŸ: {cache_file}")
    except Exception as e:
        logging.error(f"'{prefix}' ã®ãƒãƒ£ãƒ³ã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        
    return chunk_embeddings_array.tolist() if isinstance(chunk_embeddings_array, np.ndarray) else chunk_embeddings_array

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ç´„æ¬¾æ¯”è¼ƒãƒ„ãƒ¼ãƒ«",
    page_icon="",
    layout="wide"
)

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆæœŸåŒ–ï¼ˆã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ï¼‰
if 'log_initialized' not in st.session_state:
    st.session_state['log_initialized'] = True

# CSSã‚¹ã‚¿ã‚¤ãƒ«ã®é©ç”¨
def apply_custom_css():
    """ã‚«ã‚¹ã‚¿ãƒ CSSã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨"""
    css = """
    <style>
    /* ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒŠã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°èª¿æ•´ */
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 1rem;
        padding-left: 3rem;
        padding-right: 3rem;
        max-width: 1200px;
    }
    
    /* å…¨ä½“çš„ãªæ–‡å­—è‰²ã‚’æ¿ƒãã—ã¦èª­ã¿ã‚„ã™ãã™ã‚‹ */
    .stMarkdown {
        color: #333;
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .css-1d391kg {
        padding-top: 2rem;
    }
    
    /* ãƒœã‚¿ãƒ³ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚¿ã‚¤ãƒ« */
    .stButton > button {
        background-color: #004a55;
        color: white;
        border: 1px solid #004a55;
        padding: 0.5rem 1rem;
        font-weight: 500;
    }
    
    .stButton > button:hover {
        background-color: #00646f;
        border: 1px solid #00646f;
    }
    
    /* ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .stSlider .css-1cpxqw2 {
        background-color: #004a55;
    }
    
    /* ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .stSelectbox > div > div {
        border: 1px solid #e0e0e0;
    }
    
    /* ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .stFileUploader > section > div {
        border: 2px dashed #004a55;
        padding: 1rem;
    }
    
    /* ãƒãƒ«ãƒã‚»ãƒ¬ã‚¯ãƒˆã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .stMultiSelect > div > div {
        border: 1px solid #e0e0e0;
    }
    
    /* æ•°å€¤å…¥åŠ›ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .stNumberInput > div > div {
        border: 1px solid #e0e0e0;
    }
    
    /* ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .stProgress .css-1cpxqw2 {
        background-color: #004a55;
    }
    
    /* ã‚¹ãƒ”ãƒŠãƒ¼ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    .stSpinner > div {
        border-top-color: #004a55;
    }
    
    /* ãƒªãƒ³ã‚¯ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    a {
        color: #004a55;
        text-decoration: none;
    }
    
    a:hover {
        color: #00646f;
        text-decoration: underline;
    }
    
    /* ãƒ•ãƒƒã‚¿ãƒ¼ã‚’éš ã™ï¼ˆStreamlitãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ */
    footer {
        visibility: hidden;
    }
    
    /* ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ„ãƒ¼ãƒ«ãƒãƒ¼ã‚’éš ã™ */
    .stApp > header {
        background-color: transparent;
    }
    
    /* ãƒ¡ãƒ‹ãƒ¥ãƒ¼ãƒœã‚¿ãƒ³ã‚’éš ã™ */
    .stApp > div[data-testid="stDecoration"] {
        background-image: none;
    }
    
    /* å¼•ç”¨ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¹ã‚¿ã‚¤ãƒ«èª¿æ•´ */
    blockquote {
        border-left: 4px solid #004a55;
        margin: 1rem 0;
        padding-left: 1rem;
        color: #666;
        font-style: italic;
    }
    
    /* ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®ã‚¹ã‚¿ã‚¤ãƒ« */
    code {
        background-color: #f5f5f5;
        padding: 0.2rem 0.4rem;
        border-radius: 3px;
        font-family: monospace;
        color: #004a55;
        font-weight: bold;
    }
    
    /* ã‚»ã‚¯ã‚·ãƒ§ãƒ³åŒºåˆ‡ã‚Šç·šã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚¿ã‚¤ãƒ« */
    hr {
        border: none;
        height: 1px;
        background-color: #e0e0e0;
        margin: 2rem 0;
    }
    
    /* ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤ºã®æ”¹å–„ */
    .metric-container div[data-testid="metric-container"] {
        border: 1px solid #e0e0e0;
        padding: 0.5rem;
        border-radius: 4px;
    }
    </style>
    """
    st.markdown(css, unsafe_allow_html=True)

# ã‚«ã‚¹ã‚¿ãƒ CSSã‚’é©ç”¨
apply_custom_css()

st.title("ç´„æ¬¾æ¯”è¼ƒãƒ„ãƒ¼ãƒ«")

# è¨­å®šãƒ‘ãƒãƒ«
with st.sidebar:
    st.header("è¨­å®š")
    
    # ãƒãƒƒãƒãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é¸æŠ
    st.subheader("ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¨­å®š")
    
    # åˆ©ç”¨å¯èƒ½ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ãƒªã‚¹ãƒˆ
    available_algorithms = ["å¾“æ¥ã®é–¾å€¤ãƒ™ãƒ¼ã‚¹"]
    
    matching_algorithm = st.selectbox(
        "ãƒãƒƒãƒãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ",
        available_algorithms,
        index=0,
        help="å¾“æ¥ã®é–¾å€¤ãƒ™ãƒ¼ã‚¹: æ—¢å­˜ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ "
    )
    
    st.subheader("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†")
    old_cache_info = get_cache_info("old")
    new_cache_info = get_cache_info("new")
    
    # --- ãƒãƒ£ãƒ³ã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥æƒ…å ±ã®è¡¨ç¤ºã‚’è¿½åŠ  ---
    st.markdown("---") # åŒºåˆ‡ã‚Šç·š
    st.write("**ãƒãƒ£ãƒ³ã‚¯ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ã‚­ãƒ£ãƒƒã‚·ãƒ¥:**")
    cache_dir_display = CACHE_CONFIG.get("cache_directory", ".cache")
    for prefix_label, prefix_val in [("æ—§æ–‡æ›¸", "old"), ("æ–°æ–‡æ›¸", "new")]:
        chunk_emb_cache_file = os.path.join(cache_dir_display, f"{prefix_val}_chunk_embeddings.npz")
        if os.path.exists(chunk_emb_cache_file):
            try:
                file_size_bytes = os.path.getsize(chunk_emb_cache_file)
                file_size_mb = file_size_bytes / (1024 * 1024)
                st.success(f"- {prefix_label}: {file_size_mb:.2f}MB (`{os.path.basename(chunk_emb_cache_file)}`)")
            except Exception as e:
                st.warning(f"- {prefix_label}: æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼ ({e})")
        else:
            st.info(f"- {prefix_label}: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—")
    st.markdown("---") # åŒºåˆ‡ã‚Šç·š
    # --- ã“ã“ã¾ã§è¿½åŠ  ---

    st.write("**åŸºæœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥ (æ–‡åˆ†å‰²/æ–‡ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ç­‰):**") # è¡¨ç¤ºã‚’æ˜ç¢ºåŒ–
    if old_cache_info["exists"]:
        st.success(f"- æ—§æ–‡æ›¸: {old_cache_info['file_size_mb']}MB (ä¾‹: `old_sentences.json`, `old_embeddings.npz`)")
    else:
        st.info("- æ—§æ–‡æ›¸: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—")
    
    if new_cache_info["exists"]:
        st.success(f"- æ–°æ–‡æ›¸: {new_cache_info['file_size_mb']}MB (ä¾‹: `new_sentences.json`, `new_embeddings.npz`)")
    else:
        st.info("- æ–°æ–‡æ›¸: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—")
    
    if st.button("ã™ã¹ã¦ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"): 
        cache_cleared_messages = []
        error_messages = []

        files_to_delete_patterns = [
            "old_sentences.json", "old_embeddings.npz", "old_cache_metadata.json", "old_chunks.txt",
            "new_sentences.json", "new_embeddings.npz", "new_cache_metadata.json", "new_chunks.txt",
            "old_chunk_embeddings.npz", "new_chunk_embeddings.npz"
        ]
        
        cache_dir_for_clear = CACHE_CONFIG.get("cache_directory", ".cache")
        if not os.path.isdir(cache_dir_for_clear):
            st.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        else:
            for pattern in files_to_delete_patterns:
                file_path = os.path.join(cache_dir_for_clear, pattern)
                if os.path.exists(file_path):
                    try:
                        os.remove(file_path)
                        cache_cleared_messages.append(f"å‰Šé™¤: `{pattern}`")
                        logging.info(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {file_path}")
                    except Exception as e:
                        error_messages.append(f"ã‚¨ãƒ©ãƒ¼ ({pattern}): {str(e)}")
                        logging.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼ ({file_path}): {e}")
            
            if cache_cleared_messages:
                for msg in cache_cleared_messages:
                    st.success(msg)
            if error_messages:
                for msg in error_messages:
                    st.error(msg)
            
            if not cache_cleared_messages and not error_messages:
                st.info("ã‚¯ãƒªã‚¢å¯¾è±¡ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            else:
                st.success("æŒ‡å®šã•ã‚ŒãŸã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªã‚¢å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        
        st.rerun()

def check_and_load_from_cache(pdf_file, prefix):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’å¾©å…ƒ"""
    if not pdf_file:
        return None

    try:
        from doc_compare.config import CACHE_CONFIG # é–¢æ•°å†…ã§import
        import os # é–¢æ•°å†…ã§import
        cache_dir_path = CACHE_CONFIG.get("cache_directory", ".cache")
        
        if not os.path.isdir(cache_dir_path):
            logging.warning(f"({prefix}): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {cache_dir_path}")
            
        expected_metadata_file = os.path.join(cache_dir_path, f"{prefix}_cache_metadata.json")
        expected_chunks_file = os.path.join(cache_dir_path, f"{prefix}_chunks.txt")
            
    except ImportError:
        logging.error(f"({prefix}): CACHE_CONFIG ã‚’ import ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚doc_compare.config ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    except Exception as e:
        logging.error(f"({prefix}): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª/ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}")
    
    try:
        cache_info = get_cache_info(prefix)
        
        if not cache_info.get("exists", False):
            return None
        
        try:
            import json 
            import os   
            from doc_compare.config import CACHE_CONFIG 
            
            cache_dir = CACHE_CONFIG.get("cache_directory", ".cache") 
            if not cache_dir: 
                logging.error(f"({prefix}): CACHE_CONFIG ã‹ã‚‰ cache_directory ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                return None
            
            metadata_file = os.path.join(cache_dir, f"{prefix}_cache_metadata.json")
            
            if not os.path.exists(metadata_file):
                return None

            with open(metadata_file, "r", encoding="utf-8") as f:
                metadata = json.load(f)
            
            chunks_file_path = os.path.join(cache_dir, f"{prefix}_chunks.txt")

            if os.path.exists(chunks_file_path):
                chunks = []
                with open(chunks_file_path, "r", encoding="utf-8") as f:
                    content = f.read()
                
                chunk_sections = content.split("#id: ")[1:]
                if not chunk_sections: 
                    logging.warning(f"({prefix}): {prefix}_chunks.txt ã‚’splitã—ã¾ã—ãŸãŒã€æœ‰åŠ¹ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                for section_idx, section in enumerate(chunk_sections):
                    lines = section.split("\n")
                    if lines:
                        chunk_id_line = lines[0].strip()
                        chunk_id_match = re.match(r"^([a-zA-Z0-9_]+)", chunk_id_line)
                        if not chunk_id_match:
                            logging.warning(f"({prefix}): ã‚»ã‚¯ã‚·ãƒ§ãƒ³ {{section_idx}} ã§chunk_idãŒãƒ‘ãƒ¼ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸ: {{chunk_id_line}}")
                            continue
                        chunk_id = chunk_id_match.group(1)
                        
                        text_lines = []
                        for line_idx, line_content in enumerate(lines[1:]):
                            if line_content.strip() == "--------------------------------":
                                break
                            text_lines.append(line_content)
                        chunk_text = "\n".join(text_lines).strip()
                        
                        chunks.append({
                            "id": chunk_id,
                            "text": chunk_text
                        })
                
                if chunks:
                    logging.info(f"({prefix}): {prefix}_chunks.txt ã‹ã‚‰ {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å¾©å…ƒã—ã¾ã—ãŸã€‚")
                    return chunks
            
            logging.info(f"({prefix}): {prefix}_chunks.txt ãŒãªã„ã‹æœ‰åŠ¹ãªãƒãƒ£ãƒ³ã‚¯ãªã—ã€‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åŸºæœ¬æƒ…å ±ã®ã¿å¾©å…ƒè©¦è¡Œã€‚")
            chunks_info = metadata.get("chunks_info", [])
            if not chunks_info:
                logging.warning(f"({prefix}): ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã« chunks_info ãŒãªã„ã‹ç©ºã§ã™ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ã®å¾©å…ƒã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                return None
                
            chunks = []
            for chunk_info_item in chunks_info:
                chunks.append({
                    "id": chunk_info_item.get("chunk_id", "unknown_id"),
                    "text": f"[ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒ(ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿)] {{chunk_info_item.get('chunk_id', 'unknown_id')}}"
                })
            logging.info(f"({prefix}): ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯æƒ…å ±ï¼ˆIDã®ã¿ï¼‰ã‚’ {len(chunks)}å€‹å¾©å…ƒã—ã¾ã—ãŸã€‚")
            return chunks
            
        except Exception as e:
            logging.error(f"({prefix}): ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¾©å…ƒä¸­ã«è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: {e}\n{traceback.format_exc()}")
            return None
        
    except Exception as e:
        logging.error(f"({prefix}): ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯å‡¦ç†ã®äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼: {e}\n{traceback.format_exc()}")
        return None

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
col1, col2 = st.columns(2)

with col1:
    st.subheader("æ—§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
    old_pdf = st.file_uploader("æ—§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆPDFï¼‰", type=["pdf"], key="old_pdf")
    old_chunks = []
    if old_pdf:
        if "old_pdf_file_id" not in st.session_state: 
            cached_chunks = check_and_load_from_cache(old_pdf, "old")
            if cached_chunks:
                st.session_state["old_pdf_chunks"] = cached_chunks
                st.success(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰{len(cached_chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’å¾©å…ƒï¼ˆãƒãƒ£ãƒ³ã‚¯æŠ½å‡ºå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰") # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¿®æ­£
            else:
                st.session_state["old_pdf_chunks"] = extract_chunks_by_headings(old_pdf, "old")
                st.info(f"ğŸ”„ æ–°è¦ã«{len(st.session_state['old_pdf_chunks'])}ãƒãƒ£ãƒ³ã‚¯ã‚’æŠ½å‡º")
                # --- ã“ã“ã‹ã‚‰ç°¡æ˜“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å‡¦ç†ã‚’è¿½åŠ  ---
                try:
                    current_chunks = st.session_state["old_pdf_chunks"]
                    if current_chunks: # ãƒãƒ£ãƒ³ã‚¯ãŒç©ºã§ãªã„å ´åˆã®ã¿ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
                        text_hash = calculate_text_hash(current_chunks) # cache_utilã‹ã‚‰importã—ãŸé–¢æ•°ã‚’ä½¿ç”¨
                        metadata = {
                            "prefix": "old",
                            "text_hash": text_hash,
                            "total_chunks": len(current_chunks),
                            "total_sentences": 0, # ã“ã®æ®µéšã§ã¯ä¸æ˜ãªã®ã§0ã¾ãŸã¯None
                            "created_at": datetime.now().isoformat(),
                            "chunks_info": [
                                {"chunk_id": chunk["id"], "sentence_count": 0, "embedding_count": 0}
                                for chunk in current_chunks
                            ] # ãƒ€ãƒŸãƒ¼ã®chunks_infoæ§‹é€ 
                        }
                        cache_dir = CACHE_CONFIG.get("cache_directory", ".")
                        if cache_dir != "." and not os.path.exists(cache_dir):
                            os.makedirs(cache_dir, exist_ok=True) # cache_util.pyã®ä¿å­˜å‡¦ç†ã«åˆã‚ã›ã‚‹
                        
                        metadata_file_path = os.path.join(cache_dir, "old_cache_metadata.json")
                        with open(metadata_file_path, "w", encoding="utf-8") as f:
                            json.dump(metadata, f, ensure_ascii=False, indent=2)
                        logging.info(f"ç°¡æ˜“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (old_cache_metadata.json) ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {metadata_file_path}")
                    else:
                        logging.warning("æŠ½å‡ºã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ãŒç©ºã®ãŸã‚ã€old_cache_metadata.json ã®ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
                except Exception as e:
                    logging.error(f"ç°¡æ˜“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (old_cache_metadata.json) ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    st.warning(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ (old): {e}")
                # --- ã“ã“ã¾ã§ç°¡æ˜“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å‡¦ç†ã‚’è¿½åŠ  ---
            st.session_state["old_pdf_file_id"] = old_pdf.file_id
        old_chunks = st.session_state["old_pdf_chunks"]

with col2:
    st.subheader("æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
    new_pdf = st.file_uploader("æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆPDFï¼‰", type=["pdf"], key="new_pdf")
    new_chunks = []
    if new_pdf:
        if "new_pdf_file_id" not in st.session_state: 
            cached_chunks = check_and_load_from_cache(new_pdf, "new")
            if cached_chunks:
                st.session_state["new_pdf_chunks"] = cached_chunks
                st.success(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰{len(cached_chunks)}ãƒãƒ£ãƒ³ã‚¯ã‚’å¾©å…ƒï¼ˆãƒãƒ£ãƒ³ã‚¯æŠ½å‡ºå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼‰") # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä¿®æ­£
            else:
                st.session_state["new_pdf_chunks"] = extract_chunks_by_headings(new_pdf, "new")
                st.info(f"ğŸ”„ æ–°è¦ã«{len(st.session_state['new_pdf_chunks'])}ãƒãƒ£ãƒ³ã‚¯ã‚’æŠ½å‡º")
                # --- ã“ã“ã‹ã‚‰ç°¡æ˜“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å‡¦ç†ã‚’è¿½åŠ  ---
                try:
                    current_chunks = st.session_state["new_pdf_chunks"]
                    if current_chunks:
                        text_hash = calculate_text_hash(current_chunks)
                        metadata = {
                            "prefix": "new",
                            "text_hash": text_hash,
                            "total_chunks": len(current_chunks),
                            "total_sentences": 0, 
                            "created_at": datetime.now().isoformat(),
                            "chunks_info": [
                                {"chunk_id": chunk["id"], "sentence_count": 0, "embedding_count": 0}
                                for chunk in current_chunks
                            ]
                        }
                        cache_dir = CACHE_CONFIG.get("cache_directory", ".")
                        if cache_dir != "." and not os.path.exists(cache_dir):
                            os.makedirs(cache_dir, exist_ok=True)
                            
                        metadata_file_path = os.path.join(cache_dir, "new_cache_metadata.json")
                        with open(metadata_file_path, "w", encoding="utf-8") as f:
                            json.dump(metadata, f, ensure_ascii=False, indent=2)
                        logging.info(f"ç°¡æ˜“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (new_cache_metadata.json) ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {metadata_file_path}")
                    else:
                        logging.warning("æŠ½å‡ºã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ãŒç©ºã®ãŸã‚ã€new_cache_metadata.json ã®ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚")
                except Exception as e:
                    logging.error(f"ç°¡æ˜“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ (new_cache_metadata.json) ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                    st.warning(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ (new): {e}")
                # --- ã“ã“ã¾ã§ç°¡æ˜“ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜å‡¦ç†ã‚’è¿½åŠ  ---
            st.session_state["new_pdf_file_id"] = new_pdf.file_id
        new_chunks = st.session_state["new_pdf_chunks"]

# æ¯”è¼ƒè¨­å®šï¼ˆæŠ˜ã‚ŠãŸãŸã¿å¯èƒ½ï¼‰
if old_chunks and new_chunks:
    with st.expander("æ¯”è¼ƒè¨­å®š", expanded=True):
        col_settings1, col_settings2, col_settings3 = st.columns(3)
        
        with col_settings1:
            st.write("**åŸºæœ¬è¨­å®š**")
            threshold = st.slider(
                "é¡ä¼¼åº¦é–¾å€¤", 
                min_value=0.5, 
                max_value=1.0, 
                value=SIMILARITY_THRESHOLDS["default"], 
                step=0.05
            )
            max_groups = st.number_input("æœ€å¤§åˆ†æã‚°ãƒ«ãƒ¼ãƒ—æ•°", min_value=1, max_value=100, value=20, step=1)
            
        with col_settings2:
            st.write("**åŠ¹ç‡åŒ–è¨­å®š**")
            bypass_threshold = st.slider(
                "ãƒã‚¤ãƒ‘ã‚¹é–¾å€¤", 
                min_value=0.90, 
                max_value=1.0, 
                value=SIMILARITY_THRESHOLDS["bypass"], 
                step=0.01,
                help="ã“ã®å¼·åº¦ä»¥ä¸Šã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¯ã€Œå¤‰æ›´ãªã—ã€ã¨ã—ã¦è‡ªå‹•å‡¦ç†"
            )
            
            # å¼·åˆ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            force_clustering = st.checkbox(
                "å­¤ç«‹ãƒãƒ£ãƒ³ã‚¯ã®å¼·åˆ¶ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–",
                value=True,
                help="å­¤ç«‹ã—ãŸãƒãƒ£ãƒ³ã‚¯ã‚’å¼·åˆ¶çš„ã«ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼åŒ–ã™ã‚‹"
            )
            if force_clustering:
                structural_integration = st.checkbox(
                    "æ§‹é€ çš„çµ„ã¿å…¥ã‚Œ (æ¨å¥¨)",
                    value=True,
                    help="å‰å¾Œã®ãƒãƒ£ãƒ³ã‚¯ãŒæ‰€å±ã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«å­¤ç«‹ãƒãƒ£ãƒ³ã‚¯ã‚’çµ„ã¿å…¥ã‚Œã‚‹"
                )
            else:
                structural_integration = False
        
        with col_settings3:
            st.write("**ç´°åˆ†åŒ–è¨­å®š**")
            refinement_mode = st.selectbox(
                "å¤§ããªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ç´°åˆ†åŒ–ãƒ¢ãƒ¼ãƒ‰",
                options=["auto", "hierarchical", "semantic", "none"],
                index=0,
                help="auto: è‡ªå‹•é¸æŠ, hierarchical: éšå±¤çš„ç´°åˆ†åŒ–, semantic: æ„å‘³çš„ç´°åˆ†åŒ–, none: ç´°åˆ†åŒ–ãªã—"
            )
            
            # åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å½¢æˆãƒ¢ãƒ¼ãƒ‰ã‚’è¿½åŠ 
            initial_clustering_mode = st.selectbox(
                "åˆæœŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å½¢æˆãƒ¢ãƒ¼ãƒ‰",
                options=["strict", "adaptive", "relaxed"],
                index=0,
                help="strict: å³æ ¼ãªå½¢æˆ(æ¨å¥¨), adaptive: é©å¿œçš„å½¢æˆ, relaxed: å¾“æ¥æ–¹å¼"
            )
            
            # éšå±¤åˆ¶ç´„ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
            use_hierarchy_constraints = st.checkbox(
                "éšå±¤åˆ¶ç´„ã‚’ä½¿ç”¨",
                value=True,
                help="ä¸Šä½éšå±¤æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒãƒãƒ³ã‚°ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹"
            )

    # æ¯”è¼ƒå®Ÿè¡Œ
    if st.button("æ¯”è¼ƒå®Ÿè¡Œ", type="primary", use_container_width=True):
        if not old_chunks or not new_chunks:
            st.error("ä¸¡æ–¹ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            st.stop() # PDFãŒä¸¡æ–¹ãªã„å ´åˆã¯å‡¦ç†åœæ­¢
        else:
            with st.spinner("æ¯”è¼ƒå‡¦ç†ä¸­..."):
                try:
                    if old_chunks is None:
                        logging.error("old_chunks ãŒ None ã§ã™ï¼Langgraphä»¥å¤–ã®å‡¦ç†ã«é€²ã‚ã¾ã›ã‚“ã€‚")
                        st.error("æ—§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒ£ãƒ³ã‚¯ãŒæ­£ã—ãèª­ã¿è¾¼ã‚ã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚") # ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
                        st.stop() # return ã®ä»£ã‚ã‚Šã« st.stop() ã‚’ä½¿ç”¨
                    elif not old_chunks: # ç©ºã®ãƒªã‚¹ãƒˆã®å ´åˆ
                        logging.warning("old_chunks ã¯ç©ºã®ãƒªã‚¹ãƒˆã§ã™ã€‚Langgraphä»¥å¤–ã®å‡¦ç†ã«é€²ã‚ã¾ã›ã‚“ã€‚")
                        st.warning("æ—§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚") # ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘ã‚¨ãƒ©ãƒ¼è¡¨ç¤º
                        st.stop() # return ã®ä»£ã‚ã‚Šã« st.stop() ã‚’ä½¿ç”¨
                    
                    # new_chunks ã‚‚åŒæ§˜ã«ãƒã‚§ãƒƒã‚¯
                    if new_chunks is None:
                        logging.error("new_chunks ãŒ None ã§ã™ï¼Langgraphä»¥å¤–ã®å‡¦ç†ã«é€²ã‚ã¾ã›ã‚“ã€‚")
                        st.error("æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒãƒ£ãƒ³ã‚¯ãŒæ­£ã—ãèª­ã¿è¾¼ã‚ã¦ã„ã¾ã›ã‚“ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
                        st.stop() # return ã®ä»£ã‚ã‚Šã« st.stop() ã‚’ä½¿ç”¨
                    elif not new_chunks:
                        logging.warning("new_chunks ã¯ç©ºã®ãƒªã‚¹ãƒˆã§ã™ã€‚Langgraphä»¥å¤–ã®å‡¦ç†ã«é€²ã‚ã¾ã›ã‚“ã€‚")
                        st.warning("æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                        st.stop() # return ã®ä»£ã‚ã‚Šã« st.stop() ã‚’ä½¿ç”¨

                    old_vecs = get_or_create_chunk_embeddings(old_chunks, "old")
                    new_vecs = get_or_create_chunk_embeddings(new_chunks, "new")

                    # æ§‹é€ åŒ–ãƒ¡ã‚¤ãƒ³å‡¦ç†
                    structured_report, stats = process_document_comparison(
                        old_chunks, new_chunks, old_vecs, new_vecs, 
                        threshold, max_groups, refinement_mode, bypass_threshold,
                        force_clustering=force_clustering,
                        initial_clustering_mode=initial_clustering_mode,
                        structural_integration=structural_integration,
                        use_hierarchy_constraints=use_hierarchy_constraints
                    )

                    # çµæœã®æ¤œè¨¼
                    if structured_report is None:
                        st.error("æ¯”è¼ƒçµæœãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                    else:
                        st.session_state["structured_report"] = structured_report
                        st.session_state["comparison_stats"] = stats
                        st.session_state["processed_groups"] = stats.get("processed_groups_detail", [])
                        
                        # ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ãŸã‚°ãƒ«ãƒ¼ãƒ—ã®ä»¶æ•°ã‚’è¡¨ç¤º
                        error_groups = [g for g in stats.get("processed_groups_detail", []) 
                                        if "ã‚¨ãƒ©ãƒ¼" in str(g.get("structured_analysis", ""))]
                        if error_groups:
                            st.warning(f"âš ï¸ {len(error_groups)}å€‹ã®ã‚°ãƒ«ãƒ¼ãƒ—ã§åˆ†æã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚è©³ç´°ã¯ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                        
                        st.success("æ¯”è¼ƒãŒå®Œäº†ã—ã¾ã—ãŸ")
                        
                        # è‡ªå‹•å‡ºåŠ›æ©Ÿèƒ½
                        if st.session_state.get("auto_export", False):
                            try:
                                auto_export_path = export_summary_to_markdown(structured_report)
                                st.info(f"ğŸ“„ è‡ªå‹•å‡ºåŠ›å®Œäº†: {os.path.basename(auto_export_path)}")
                            except Exception as e:
                                st.warning(f"âš ï¸ è‡ªå‹•å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
                except Exception as e:
                    st.error(f"æ¯”è¼ƒå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
                    logging.error(f"ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
                    with st.expander("ã‚¨ãƒ©ãƒ¼è©³ç´°"):
                        st.code(str(e))
                        st.write("**ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—:**", type(e).__name__)
                        import traceback
                        st.code(traceback.format_exc())

# çµæœè¡¨ç¤ºï¼ˆæŠ˜ã‚ŠãŸãŸã¿å¯èƒ½ï¼‰
if "structured_report" in st.session_state and "comparison_stats" in st.session_state:
    structured_report = st.session_state["structured_report"]
    stats = st.session_state["comparison_stats"]
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³
    with st.expander("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›", expanded=False):
        st.subheader("æ¯”è¼ƒçµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›")
        
        col_export1, col_export2, col_export3 = st.columns(3)
        
        with col_export1:
            st.write("**å˜ä¸€å½¢å¼å‡ºåŠ›**")
            if st.button("ğŸ“„ Markdownè©³ç´°ç‰ˆ", use_container_width=True):
                try:
                    file_path = export_to_markdown(structured_report)
                    st.success(f"âœ… å‡ºåŠ›å®Œäº†: {file_path}")
                except Exception as e:
                    st.error(f"âŒ å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
            if st.button("ğŸ“‹ Markdownã‚µãƒãƒªãƒ¼", use_container_width=True):
                try:
                    file_path = export_summary_to_markdown(structured_report)
                    st.success(f"âœ… å‡ºåŠ›å®Œäº†: {file_path}")
                except Exception as e:
                    st.error(f"âŒ å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        with col_export2:
            st.write("**ãƒ‡ãƒ¼ã‚¿å½¢å¼å‡ºåŠ›**")
            if st.button("ğŸ“Š JSONå½¢å¼", use_container_width=True):
                try:
                    file_path = export_to_json(structured_report)
                    st.success(f"âœ… å‡ºåŠ›å®Œäº†: {file_path}")
                except Exception as e:
                    st.error(f"âŒ å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
            
            if st.button("ğŸ“ˆ CSVå½¢å¼", use_container_width=True):
                try:
                    file_path = export_to_csv(structured_report)
                    st.success(f"âœ… å‡ºåŠ›å®Œäº†: {file_path}")
                except Exception as e:
                    st.error(f"âŒ å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        with col_export3:
            st.write("**ä¸€æ‹¬å‡ºåŠ›**")
            if st.button("ğŸ¯ ã™ã¹ã¦ã®å½¢å¼ã§å‡ºåŠ›", type="primary", use_container_width=True):
                try:
                    with st.spinner("å„å½¢å¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ä¸­..."):
                        results = export_all_formats(structured_report)
                        
                        st.success("âœ… ã™ã¹ã¦ã®å½¢å¼ã§å‡ºåŠ›ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                        
                        # å‡ºåŠ›ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§ã‚’è¡¨ç¤º
                        st.write("**å‡ºåŠ›ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«:**")
                        for format_type, file_path in results.items():
                            file_name = os.path.basename(file_path)
                            st.write(f"- **{format_type}**: `{file_name}`")
                        
                        st.info(f"ğŸ“ å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€: `output/`")
                        
                except Exception as e:
                    st.error(f"âŒ ä¸€æ‹¬å‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        # å‡ºåŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³
        st.markdown("---")
        col_options1, col_options2 = st.columns(2)
        
        with col_options1:
            st.write("**å‡ºåŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³**")
            auto_export = st.checkbox(
                "æ¯”è¼ƒå®Œäº†æ™‚ã«è‡ªå‹•ã§Markdownã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›", 
                value=False,
                help="æ¯”è¼ƒå‡¦ç†ãŒå®Œäº†ã™ã‚‹ã¨è‡ªå‹•çš„ã«Markdownã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›ã—ã¾ã™"
            )
            
            if auto_export:
                st.session_state["auto_export"] = True
            else:
                st.session_state["auto_export"] = False
        
        with col_options2:
            st.write("**å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€æƒ…å ±**")
            if os.path.exists("output"):
                output_files = [f for f in os.listdir("output") if f.endswith(('.md', '.json', '.csv'))]
                st.info(f"ğŸ“ output ãƒ•ã‚©ãƒ«ãƒ€: {len(output_files)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«")
                
                if st.button("ğŸ—‚ï¸ å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã"):
                    try:
                        import subprocess
                        subprocess.run(["explorer", os.path.abspath("output")], shell=True)
                    except:
                        st.info("ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ‰‹å‹•ã§é–‹ã„ã¦ãã ã•ã„: `output/`")
            else:
                st.info("ğŸ“ output ãƒ•ã‚©ãƒ«ãƒ€ã¯ã¾ã ä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
    
    # ãƒ•ã‚£ãƒ«ã‚¿è¨­å®š
    col_filter1, col_filter2 = st.columns([3, 1])
    with col_filter1:
        # å¤‰æ›´ã®ç¨®é¡åˆ¥ã®ä»¶æ•°ã‚’è¨ˆç®—
        change_type_counts = {}
        for group in structured_report.groups:
            change_type = group.analysis.change_type
            change_type_counts[change_type] = change_type_counts.get(change_type, 0) + 1
        
        # è¿½åŠ ãƒ»å‰Šé™¤ãƒãƒ£ãƒ³ã‚¯ã®ä»¶æ•°ã‚‚è¿½åŠ 
        change_type_counts["è¿½åŠ "] = change_type_counts.get("è¿½åŠ ", 0) + len(structured_report.added_chunks)
        change_type_counts["å‰Šé™¤"] = change_type_counts.get("å‰Šé™¤", 0) + len(structured_report.deleted_chunks)
        
        # ãƒ•ã‚£ãƒ«ã‚¿é¸æŠè‚¢ã‚’ä»¶æ•°ä»˜ãã§ä½œæˆ
        filter_options = ["ã™ã¹ã¦"]
        for change_type in ["å¤‰æ›´", "è¿½åŠ ", "å‰Šé™¤", "å¤‰æ›´ãªã—", "ãã®ä»–"]:
            count = change_type_counts.get(change_type, 0)
            if count > 0:
                filter_options.append(f"{change_type} ({count})")
            else:
                filter_options.append(change_type)
        
        selected_filters = st.multiselect(
            "å¤‰æ›´ã®ç¨®é¡ã§ãƒ•ã‚£ãƒ«ã‚¿",
            filter_options,
            default=["ã™ã¹ã¦"],
            help="è¡¨ç¤ºã™ã‚‹å¤‰æ›´ã®ç¨®é¡ã‚’é¸æŠã—ã¦ãã ã•ã„"
        )
        
        # é¸æŠã•ã‚ŒãŸãƒ•ã‚£ãƒ«ã‚¿ã‹ã‚‰å®Ÿéš›ã®å¤‰æ›´ã®ç¨®é¡ã‚’æŠ½å‡º
        change_type_filter = []
        for filter_item in selected_filters:
            if filter_item == "ã™ã¹ã¦":
                change_type_filter.append("ã™ã¹ã¦")
            else:
                # " (æ•°)" ã®éƒ¨åˆ†ã‚’é™¤å»
                change_type = filter_item.split(" (")[0]
                change_type_filter.append(change_type)
    
    with col_filter2:
        if st.button("ãƒ•ã‚£ãƒ«ã‚¿ãƒªã‚»ãƒƒãƒˆ"):
            st.rerun()

    # è©³ç´°åˆ†æçµæœï¼ˆæ¦‚è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³å¤–ï¼‰
    if structured_report:
        filtered_report = filter_report_by_change_types(structured_report, change_type_filter)
        render_comparison_report(filtered_report)
    else:
        st.info("é¸æŠã—ãŸãƒ•ã‚£ãƒ«ã‚¿ã«è©²å½“ã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

# ãƒ‡ãƒãƒƒã‚°æ©Ÿèƒ½
with st.expander("ãƒ‡ãƒãƒƒã‚°æƒ…å ±"):
    col_debug1, col_debug2, col_debug3 = st.columns(3)
    
    with col_debug1:
        if st.button("ã‚°ãƒ«ãƒ¼ãƒ—ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°"):
            try:
                with open("group_debug.log", "r", encoding="utf-8") as f:
                    st.text_area("group_debug.log", f.read(), height=200)
            except FileNotFoundError:
                st.warning("ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    
    with col_debug2:
        if st.button("é¡ä¼¼åº¦ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°"):
            try:
                with open("similarity_debug.log", "r", encoding="utf-8") as f:
                    st.text_area("similarity_debug.log", f.read(), height=300)
            except FileNotFoundError:
                st.warning("é¡ä¼¼åº¦ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    
    with col_debug3:
        if st.button("ã‚°ãƒ«ãƒ¼ãƒ—ã‚¿ã‚¤ãƒ—ãƒ‡ãƒãƒƒã‚°"):
            if "structured_report" in st.session_state:
                structured_report = st.session_state["structured_report"]
                
                st.write("**ã‚°ãƒ«ãƒ¼ãƒ—ã‚¿ã‚¤ãƒ—ã®è©³ç´°æƒ…å ±:**")
                group_type_counts = {}
                
                for group in structured_report.groups:
                    group_type = group.group_type
                    group_type_counts[group_type] = group_type_counts.get(group_type, 0) + 1
                    
                    # æœ€åˆã®5å€‹ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«ã¤ã„ã¦è©³ç´°ã‚’è¡¨ç¤º
                    if group.group_number <= 5:
                        st.write(f"ã‚°ãƒ«ãƒ¼ãƒ— {group.group_number}:")
                        st.write(f"  - ã‚°ãƒ«ãƒ¼ãƒ—ã‚¿ã‚¤ãƒ—: {group.group_type}")
                        st.write(f"  - æ—§ãƒãƒ£ãƒ³ã‚¯æ•°: {len(group.old_chunks)}")
                        st.write(f"  - æ–°ãƒãƒ£ãƒ³ã‚¯æ•°: {len(group.new_chunks)}")
                        st.write(f"  - å¤‰æ›´ã®ç¨®é¡: {group.analysis.change_type}")
                        st.write(f"  - é¡ä¼¼åº¦å¼·åº¦: {group.strength:.4f}")
                        st.write("---")
                
                st.write("**ã‚°ãƒ«ãƒ¼ãƒ—ã‚¿ã‚¤ãƒ—åˆ¥é›†è¨ˆ:**")
                for group_type, count in group_type_counts.items():
                    st.write(f"- {group_type}: {count}å€‹")
                
                st.write("**æœŸå¾…ã•ã‚Œã‚‹ã‚°ãƒ«ãƒ¼ãƒ—ã‚¿ã‚¤ãƒ—ã¨ãã®æ¡ä»¶:**")
                st.write("- 1:1: æ—§ãƒãƒ£ãƒ³ã‚¯1å€‹ â†’ æ–°ãƒãƒ£ãƒ³ã‚¯1å€‹")
                st.write("- 1:N: æ—§ãƒãƒ£ãƒ³ã‚¯1å€‹ â†’ æ–°ãƒãƒ£ãƒ³ã‚¯è¤‡æ•°å€‹")
                st.write("- N:1: æ—§ãƒãƒ£ãƒ³ã‚¯è¤‡æ•°å€‹ â†’ æ–°ãƒãƒ£ãƒ³ã‚¯1å€‹")
                st.write("- N:N: æ—§ãƒãƒ£ãƒ³ã‚¯è¤‡æ•°å€‹ â†’ æ–°ãƒãƒ£ãƒ³ã‚¯è¤‡æ•°å€‹")
            else:
                st.info("çµæœãŒã¾ã ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    
    st.markdown("---")
    col_debug4, col_debug5 = st.columns(2)
    
    with col_debug4:
        if st.button("ã‚°ãƒ«ãƒ¼ãƒ—ã‚¿ã‚¤ãƒ—ãƒ†ã‚¹ãƒˆ"):
            # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ
            from doc_compare.structured_models import GroupAnalysisResult, ChunkInfo, ComparisonReport, AnalysisResult, CorrespondenceInfo
            from doc_compare.visualization import create_sankey_diagram
            
            # ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ£ãƒ³ã‚¯æƒ…å ±
            sample_old_1 = ChunkInfo(id="old_chunk_1", content="ã‚µãƒ³ãƒ—ãƒ«æ—§ãƒãƒ£ãƒ³ã‚¯1", heading="è¦‹å‡ºã—1")
            sample_old_2 = ChunkInfo(id="old_chunk_2", content="ã‚µãƒ³ãƒ—ãƒ«æ—§ãƒãƒ£ãƒ³ã‚¯2", heading="è¦‹å‡ºã—2")
            sample_old_3 = ChunkInfo(id="old_chunk_3", content="ã‚µãƒ³ãƒ—ãƒ«æ—§ãƒãƒ£ãƒ³ã‚¯3", heading="è¦‹å‡ºã—3")
            
            sample_new_1 = ChunkInfo(id="new_chunk_1", content="ã‚µãƒ³ãƒ—ãƒ«æ–°ãƒãƒ£ãƒ³ã‚¯1", heading="è¦‹å‡ºã—1")
            sample_new_2 = ChunkInfo(id="new_chunk_2", content="ã‚µãƒ³ãƒ—ãƒ«æ–°ãƒãƒ£ãƒ³ã‚¯2", heading="è¦‹å‡ºã—2")
            sample_new_3 = ChunkInfo(id="new_chunk_3", content="ã‚µãƒ³ãƒ—ãƒ«æ–°ãƒãƒ£ãƒ³ã‚¯3", heading="è¦‹å‡ºã—3")
            sample_new_4 = ChunkInfo(id="new_chunk_4", content="ã‚µãƒ³ãƒ—ãƒ«æ–°ãƒãƒ£ãƒ³ã‚¯4", heading="è¦‹å‡ºã—4")
            
            # ã‚µãƒ³ãƒ—ãƒ«åˆ†æçµæœ
            sample_analysis = AnalysisResult(
                change_type="å¤‰æ›´",
                summary="ãƒ†ã‚¹ãƒˆç”¨ã®åˆ†æçµæœ",
                detailed_analysis="è©³ç´°ãªåˆ†æå†…å®¹",
                main_changes=["å¤‰æ›´ç‚¹1", "å¤‰æ›´ç‚¹2"],
                correspondence_details="å¯¾å¿œé–¢ä¿‚ã®è©³ç´°"
            )
            
            # ã‚µãƒ³ãƒ—ãƒ«å¯¾å¿œé–¢ä¿‚
            sample_correspondence_1_1 = CorrespondenceInfo(
                old_chunk_ids=["old_chunk_1"],
                new_chunk_ids=["new_chunk_1"],
                correspondence_type="1:1"
            )
            
            sample_correspondence_1_n = CorrespondenceInfo(
                old_chunk_ids=["old_chunk_2"],
                new_chunk_ids=["new_chunk_2", "new_chunk_3"],
                correspondence_type="1:N"
            )
            
            sample_correspondence_n_1 = CorrespondenceInfo(
                old_chunk_ids=["old_chunk_2", "old_chunk_3"],
                new_chunk_ids=["new_chunk_2"],
                correspondence_type="N:1"
            )
            
            sample_correspondence_n_n = CorrespondenceInfo(
                old_chunk_ids=["old_chunk_2", "old_chunk_3"],
                new_chunk_ids=["new_chunk_3", "new_chunk_4"],
                correspondence_type="N:N"
            )
            
            # ãƒ†ã‚¹ãƒˆç”¨ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆ
            test_groups = [
                GroupAnalysisResult(
                    group_number=1,
                    group_type="1:1",
                    old_chunks=[sample_old_1],
                    new_chunks=[sample_new_1],
                    strength=0.95,
                    refinement_method="original",
                    processing_method="bypassed",
                    analysis=sample_analysis,
                    correspondence=sample_correspondence_1_1
                ),
                GroupAnalysisResult(
                    group_number=2,
                    group_type="1:N",
                    old_chunks=[sample_old_2],
                    new_chunks=[sample_new_2, sample_new_3],
                    strength=0.85,
                    refinement_method="hierarchical_core",
                    processing_method="gpt_analyzed",
                    analysis=sample_analysis,
                    correspondence=sample_correspondence_1_n
                ),
                GroupAnalysisResult(
                    group_number=3,
                    group_type="N:1",
                    old_chunks=[sample_old_2, sample_old_3],
                    new_chunks=[sample_new_2],
                    strength=0.78,
                    refinement_method="semantic",
                    processing_method="gpt_analyzed",
                    analysis=sample_analysis,
                    correspondence=sample_correspondence_n_1
                ),
                GroupAnalysisResult(
                    group_number=4,
                    group_type="N:N",
                    old_chunks=[sample_old_2, sample_old_3],
                    new_chunks=[sample_new_3, sample_new_4],
                    strength=0.72,
                    refinement_method="hierarchical_remaining",
                    processing_method="gpt_analyzed",
                    analysis=sample_analysis,
                    correspondence=sample_correspondence_n_n
                )
            ]
            
            # ãƒ†ã‚¹ãƒˆç”¨ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
            test_report = ComparisonReport(
                summary={"processed_groups": 4, "added_chunks": 0, "deleted_chunks": 0, "bypassed_groups": 1, "gpt_analyzed_groups": 3},
                groups=test_groups,
                added_chunks=[],
                deleted_chunks=[]
            )
            
            st.write("**ãƒ†ã‚¹ãƒˆç”¨ã‚µãƒ³ã‚­ãƒ¼ãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ï¼ˆå…¨ã‚°ãƒ«ãƒ¼ãƒ—ã‚¿ã‚¤ãƒ—å«ã‚€ï¼‰:**")
            test_fig = create_sankey_diagram(test_report)
            st.plotly_chart(test_fig, use_container_width=True, config={'displayModeBar': False})
            
            st.write("**ãƒ†ã‚¹ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®è©³ç´°:**")
            for group in test_groups:
                st.write(f"- ã‚°ãƒ«ãƒ¼ãƒ— {group.group_number}: {group.group_type} (æ—§:{len(group.old_chunks)}, æ–°:{len(group.new_chunks)})")
    
    with col_debug5:
        if st.button("å¤‰æ›´ç¨®é¡ãƒ‡ãƒãƒƒã‚°"):
            if "structured_report" in st.session_state:
                structured_report = st.session_state["structured_report"]
                
                st.write("**æ§‹é€ åŒ–åˆ†æçµæœã®å¤‰æ›´ç¨®é¡:**")
                for group in structured_report.groups:
                    st.write(f"ã‚°ãƒ«ãƒ¼ãƒ— {group.group_number}: {group.analysis.change_type}")
                
                st.write("**ãƒ•ã‚£ãƒ«ã‚¿æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ:**")
                test_filters = ["å¤‰æ›´", "è¿½åŠ ", "å‰Šé™¤", "å¤‰æ›´ãªã—"]
                for test_filter in test_filters:
                    filtered_report = filter_report_by_change_types(structured_report, [test_filter])
                    st.write(f"{test_filter}ãƒ•ã‚£ãƒ«ã‚¿: {len(filtered_report.groups)}ã‚°ãƒ«ãƒ¼ãƒ—")
            else:
                st.info("çµæœãŒã¾ã ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        
        if st.button("ç¾åœ¨ã®çŠ¶æ…‹æƒ…å ±"):
            if "structured_report" in st.session_state:
                st.write("**æ§‹é€ åŒ–ãƒ¬ãƒãƒ¼ãƒˆ**: åˆ©ç”¨å¯èƒ½")
            else:
                st.write("**æ§‹é€ åŒ–ãƒ¬ãƒãƒ¼ãƒˆ**: ã¾ã ç”Ÿæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")
