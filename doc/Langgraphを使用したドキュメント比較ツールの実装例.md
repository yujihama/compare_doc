# Langgraphã‚’ä½¿ç”¨ã—ãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¯”è¼ƒãƒ„ãƒ¼ãƒ«ã®å®Ÿè£…ä¾‹

ä»¥ä¸‹ã¯ã€Langgraphã‚’ä½¿ç”¨ã—ã¦2ã¤ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¯”è¼ƒã™ã‚‹ãƒ„ãƒ¼ãƒ«ã®å®Ÿè£…ä¾‹ã§ã™ã€‚ã“ã®ä¾‹ã§ã¯ã€GPT-4.1-miniã¨OpenAIã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆé–“ã®æ„å‘³çš„ãªæ¯”è¼ƒã‚’è¡Œã„ã¾ã™ã€‚

```python
import os
from typing import List, Dict, Optional, TypedDict, Literal, Any, Tuple
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import numpy as np
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
import json

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
os.environ["OPENAI_API_KEY"] = "your-api-key-here"

# çŠ¶æ…‹ã®å‹å®šç¾©
class ComparisonState(TypedDict):
    # å…¥åŠ›ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    document_a: List[Dict]
    document_b: List[Dict]
    
    # ç¾åœ¨å‡¦ç†ä¸­ã®ãƒãƒ£ãƒ³ã‚¯
    current_chunk_a: Optional[Dict]
    
    # æ¯”è¼ƒçµæœã®è“„ç©
    comparison_results: List[Dict]
    
    # å‡¦ç†æ¸ˆã¿ãƒãƒ£ãƒ³ã‚¯ã®è¿½è·¡
    processed_chunks_a: List[str]
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ€è€ƒãƒ­ã‚°
    agent_thoughts: List[str]
    
    # ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—
    current_step: str

# ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
llm = ChatOpenAI(model="gpt-4.1-mini")
embeddings = OpenAIEmbeddings(model="text-embedding-large")

# ãƒ„ãƒ¼ãƒ«1: æ–‡å­—åˆ—æ¤œç´¢ãƒ„ãƒ¼ãƒ«
def string_search_tool(query: str, document: List[Dict]) -> List[Dict]:
    """
    æŒ‡å®šã•ã‚ŒãŸèªå¥ã§æ–‡å­—åˆ—æ¤œç´¢ã—ãŸçµæœã‚’å–å¾—ã™ã‚‹
    
    Args:
        query: æ¤œç´¢èªå¥
        document: æ¤œç´¢å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        
    Returns:
        ãƒãƒƒãƒã—ãŸãƒãƒ£ãƒ³ã‚¯ã®ãƒªã‚¹ãƒˆ
    """
    results = []
    keywords = query.split() if isinstance(query, str) else query
    
    for chunk in document:
        content = chunk["content"].lower()
        matches = [keyword for keyword in keywords if keyword.lower() in content]
        if matches:
            results.append({
                "id": chunk["id"],
                "content": chunk["content"],
                "matched_keywords": matches,
                "match_count": len(matches)
            })
    
    # ãƒãƒƒãƒæ•°ã®å¤šã„é †ã«ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x["match_count"], reverse=True)
    return results

# ãƒ„ãƒ¼ãƒ«2: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢ãƒ„ãƒ¼ãƒ«
def vector_similarity_tool(
    query: str, 
    document: List[Dict], 
    threshold: float = 0.7, 
    max_results: int = 5
) -> List[Dict]:
    """
    æŒ‡å®šã•ã‚ŒãŸèªå¥ãƒ»æ–‡ç« ã¨ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ãŒé«˜ã„ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ã™ã‚‹
    
    Args:
        query: ã‚¯ã‚¨ãƒªãƒ†ã‚­ã‚¹ãƒˆ
        document: æ¤œç´¢å¯¾è±¡ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        threshold: é¡ä¼¼åº¦é–¾å€¤
        max_results: æœ€å¤§çµæœæ•°
        
    Returns:
        é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ä»˜ãã®ãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
    """
    # ã‚¯ã‚¨ãƒªã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
    query_embedding = embeddings.embed_query(query)
    
    results = []
    for chunk in document:
        # ãƒãƒ£ãƒ³ã‚¯å†…å®¹ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
        chunk_embedding = embeddings.embed_query(chunk["content"])
        
        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®è¨ˆç®—
        similarity = np.dot(query_embedding, chunk_embedding) / (
            np.linalg.norm(query_embedding) * np.linalg.norm(chunk_embedding)
        )
        
        if similarity >= threshold:
            results.append({
                "id": chunk["id"],
                "content": chunk["content"],
                "similarity": float(similarity)
            })
    
    # é¡ä¼¼åº¦ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
    results.sort(key=lambda x: x["similarity"], reverse=True)
    return results[:max_results]

# ãƒ„ãƒ¼ãƒ«3: æ¯”è¼ƒå…ƒãƒãƒ£ãƒ³ã‚¯ã®æ–‡è„ˆå–å¾—ãƒ„ãƒ¼ãƒ«
def get_context_tool_a(chunk_id: str, document: List[Dict]) -> Dict:
    """
    æ¯”è¼ƒå…ƒãƒãƒ£ãƒ³ã‚¯ã®æ–‡è„ˆæƒ…å ±ã‚’å–å¾—ã™ã‚‹
    
    Args:
        chunk_id: ãƒãƒ£ãƒ³ã‚¯ID
        document: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        
    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®è©³ç´°æƒ…å ±ã¨é–¢é€£ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    """
    for chunk in document:
        if chunk["id"] == chunk_id:
            return chunk
    return None

# ãƒ„ãƒ¼ãƒ«4: æ¯”è¼ƒå…ˆãƒãƒ£ãƒ³ã‚¯ã®æ–‡è„ˆå–å¾—ãƒ„ãƒ¼ãƒ«
def get_context_tool_b(chunk_id: str, document: List[Dict]) -> Dict:
    """
    æ¯”è¼ƒå…ˆãƒãƒ£ãƒ³ã‚¯ã®æ–‡è„ˆæƒ…å ±ã‚’å–å¾—ã™ã‚‹
    
    Args:
        chunk_id: ãƒãƒ£ãƒ³ã‚¯ID
        document: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        
    Returns:
        ãƒãƒ£ãƒ³ã‚¯ã®è©³ç´°æƒ…å ±ã¨é–¢é€£ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    """
    for chunk in document:
        if chunk["id"] == chunk_id:
            return chunk
    return None

# ãƒ„ãƒ¼ãƒ«5: å‰å¾Œãƒãƒ£ãƒ³ã‚¯å–å¾—ãƒ„ãƒ¼ãƒ«
def get_adjacent_chunks_tool(
    chunk_id: str, 
    document: List[Dict], 
    direction: Literal["prev", "next"], 
    count: int = 1
) -> List[Dict]:
    """
    ç‰¹å®šã®ãƒãƒ£ãƒ³ã‚¯ã®å‰ã€ã¾ãŸã¯å¾Œã«ç¶šããƒãƒ£ãƒ³ã‚¯ã‚’æŒ‡å®šã•ã‚ŒãŸæ•°ã ã‘å–å¾—
    
    Args:
        chunk_id: ãƒãƒ£ãƒ³ã‚¯ID
        document: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        direction: å–å¾—æ–¹å‘ï¼ˆ"prev"ã¾ãŸã¯"next"ï¼‰
        count: å–å¾—æ•°
        
    Returns:
        æŒ‡å®šã•ã‚ŒãŸæ•°ã®å‰å¾Œãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
    """
    # ãƒãƒ£ãƒ³ã‚¯ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç‰¹å®š
    chunk_index = -1
    for i, chunk in enumerate(document):
        if chunk["id"] == chunk_id:
            chunk_index = i
            break
    
    if chunk_index == -1:
        return []
    
    if direction == "prev":
        start_idx = max(0, chunk_index - count)
        return document[start_idx:chunk_index]
    else:  # direction == "next"
        end_idx = min(len(document), chunk_index + count + 1)
        return document[chunk_index + 1:end_idx]

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºé–¢æ•°
def extract_keywords(text: str) -> List[str]:
    """
    ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹
    
    Args:
        text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ
        
    Returns:
        æŠ½å‡ºã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ã‚ãªãŸã¯ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"),
        ("user", "ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’5-10å€‹æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¿”ã—ã¦ãã ã•ã„ã€‚\n\n{text}")
    ])
    
    chain = prompt | llm | StrOutputParser()
    keywords = chain.invoke({"text": text})
    return [kw.strip() for kw in keywords.split(",")]

# æ¯”è¼ƒé–¢æ•°
def compare_chunks(chunk_a: Dict, candidates_b: List[Dict]) -> Dict:
    """
    ãƒãƒ£ãƒ³ã‚¯Aã¨å€™è£œãƒãƒ£ãƒ³ã‚¯Bã‚’æ¯”è¼ƒã—ã€å·®åˆ†ã‚’ç‰¹å®šã™ã‚‹
    
    Args:
        chunk_a: æ¯”è¼ƒå…ƒãƒãƒ£ãƒ³ã‚¯
        candidates_b: æ¯”è¼ƒå…ˆã®å€™è£œãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆ
        
    Returns:
        æ¯”è¼ƒçµæœï¼ˆè¿½åŠ /å‰Šé™¤/å¤‰æ›´ï¼‰ã¨è©³ç´°æƒ…å ±
    """
    SIMILARITY_THRESHOLD = 0.75
    
    if not candidates_b:
        # é¡ä¼¼ãƒãƒ£ãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å‰Šé™¤ã¨åˆ¤æ–­
        return {
            "type": "å‰Šé™¤",
            "chunk_a": chunk_a,
            "chunk_b": None,
            "details": "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆBã«å¯¾å¿œã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
        }
    
    # æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„å€™è£œã‚’ç‰¹å®š
    best_match = candidates_b[0]
    
    # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ãŒé–¾å€¤ä»¥ä¸Šã®å ´åˆ
    if "similarity" in best_match and best_match["similarity"] >= SIMILARITY_THRESHOLD:
        # å†…å®¹ãŒå®Œå…¨ã«ä¸€è‡´ã™ã‚‹å ´åˆ
        if chunk_a["content"] == best_match["content"]:
            return {
                "type": "ä¸€è‡´",
                "chunk_a": chunk_a,
                "chunk_b": best_match,
                "details": "å†…å®¹ãŒå®Œå…¨ã«ä¸€è‡´ã—ã¦ã„ã¾ã™"
            }
        else:
            # å†…å®¹ãŒé¡ä¼¼ã—ã¦ã„ã‚‹ãŒå®Œå…¨ä¸€è‡´ã§ã¯ãªã„å ´åˆã¯å¤‰æ›´ã¨åˆ¤æ–­
            # å¤‰æ›´å†…å®¹ã®è©³ç´°ã‚’å–å¾—
            prompt = ChatPromptTemplate.from_messages([
                ("system", "ã‚ãªãŸã¯2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆé–“ã®å¤‰æ›´ç‚¹ã‚’ç‰¹å®šã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"),
                ("user", "ä»¥ä¸‹ã®2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¯”è¼ƒã—ã€å¤‰æ›´ç‚¹ã‚’ç®‡æ¡æ›¸ãã§è©³ç´°ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚\n\nãƒ†ã‚­ã‚¹ãƒˆA:\n{text_a}\n\nãƒ†ã‚­ã‚¹ãƒˆB:\n{text_b}")
            ])
            
            chain = prompt | llm | StrOutputParser()
            changes = chain.invoke({
                "text_a": chunk_a["content"],
                "text_b": best_match["content"]
            })
            
            return {
                "type": "å¤‰æ›´",
                "chunk_a": chunk_a,
                "chunk_b": best_match,
                "details": changes,
                "similarity": best_match.get("similarity", 0)
            }
    else:
        # é¡ä¼¼åº¦ãŒä½ã„å ´åˆã¯å‰Šé™¤ã¨åˆ¤æ–­ã—ã€æœ€ã‚‚è¿‘ã„ã‚‚ã®ã‚’å‚è€ƒæƒ…å ±ã¨ã—ã¦æä¾›
        return {
            "type": "å‰Šé™¤",
            "chunk_a": chunk_a,
            "chunk_b": best_match if "similarity" in best_match else None,
            "details": f"é¡ä¼¼åº¦ãŒä½ã„ãŸã‚å‰Šé™¤ã¨åˆ¤æ–­ (é¡ä¼¼åº¦: {best_match.get('similarity', 0)})",
            "similarity": best_match.get("similarity", 0)
        }

# Reactã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…
def react_agent(state: ComparisonState) -> ComparisonState:
    """
    Reactã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè£…
    
    Args:
        state: ç¾åœ¨ã®æ¯”è¼ƒçŠ¶æ…‹
        
    Returns:
        æ›´æ–°ã•ã‚ŒãŸæ¯”è¼ƒçŠ¶æ…‹
    """
    # ç¾åœ¨ã®ãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—
    current_chunk = state["current_chunk_a"]
    
    # æ€è€ƒãƒ­ã‚°ã‚’åˆæœŸåŒ–
    thoughts = []
    
    # è¦³å¯Ÿ: ç¾åœ¨ã®çŠ¶æ…‹ã‚’è¦³å¯Ÿ
    thoughts.append(f"ç¾åœ¨å‡¦ç†ä¸­ã®ãƒãƒ£ãƒ³ã‚¯: {current_chunk['id']}")
    thoughts.append(f"ãƒãƒ£ãƒ³ã‚¯å†…å®¹: {current_chunk['content'][:100]}...")
    
    # æ€è€ƒ: æ¯”è¼ƒæˆ¦ç•¥ã‚’ç«‹ã¦ã‚‹
    thoughts.append("é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦æ¤œç´¢ã‚’è¡Œã„ã¾ã™")
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
    keywords = extract_keywords(current_chunk["content"])
    thoughts.append(f"æŠ½å‡ºã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords)}")
    
    # ãƒ„ãƒ¼ãƒ«é¸æŠã¨å®Ÿè¡Œ: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢
    thoughts.append("ãƒ„ãƒ¼ãƒ«1ï¼ˆæ–‡å­—åˆ—æ¤œç´¢ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¾ã™")
    keyword_results = string_search_tool(keywords, state["document_b"])
    
    if not keyword_results:
        thoughts.append("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã§ã¯çµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢ã‚’è©¦ã¿ã¾ã™")
        # ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦æ¤œç´¢
        similarity_results = vector_similarity_tool(
            current_chunk["content"], 
            state["document_b"],
            threshold=0.7,
            max_results=5
        )
        candidates = similarity_results
    else:
        candidates = keyword_results
        
    thoughts.append(f"å€™è£œãƒãƒ£ãƒ³ã‚¯æ•°: {len(candidates)}")
    
    # æ–‡è„ˆå–å¾—
    if candidates:
        thoughts.append("å€™è£œãƒãƒ£ãƒ³ã‚¯ã®æ–‡è„ˆã‚’å–å¾—ã—ã¾ã™")
        for i, candidate in enumerate(candidates[:3]):  # ä¸Šä½3ä»¶ã®ã¿å‡¦ç†
            context = get_context_tool_b(candidate["id"], state["document_b"])
            if context:
                candidates[i]["context"] = context
            
        # å‰å¾Œãƒãƒ£ãƒ³ã‚¯å–å¾—ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        if len(candidates) > 0:
            best_candidate = candidates[0]
            thoughts.append(f"æœ€è‰¯å€™è£œãƒãƒ£ãƒ³ã‚¯ {best_candidate['id']} ã®å‰å¾Œãƒãƒ£ãƒ³ã‚¯ã‚’å–å¾—ã—ã¾ã™")
            prev_chunks = get_adjacent_chunks_tool(
                best_candidate["id"], 
                state["document_b"], 
                direction="prev", 
                count=1
            )
            next_chunks = get_adjacent_chunks_tool(
                best_candidate["id"], 
                state["document_b"], 
                direction="next", 
                count=1
            )
            
    # æ¯”è¼ƒçµæœã®æ±ºå®š
    comparison_result = compare_chunks(current_chunk, candidates)
    thoughts.append(f"æ¯”è¼ƒçµæœ: {comparison_result['type']}")
    
    # çŠ¶æ…‹ã®æ›´æ–°
    state["comparison_results"].append(comparison_result)
    state["processed_chunks_a"].append(current_chunk["id"])
    state["agent_thoughts"].extend(thoughts)
    
    return state

# åˆæœŸåŒ–é–¢æ•°
def initialize_comparison(state: Optional[ComparisonState] = None) -> ComparisonState:
    """
    æ¯”è¼ƒçŠ¶æ…‹ã‚’åˆæœŸåŒ–ã™ã‚‹
    
    Args:
        state: åˆæœŸçŠ¶æ…‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
    Returns:
        åˆæœŸåŒ–ã•ã‚ŒãŸæ¯”è¼ƒçŠ¶æ…‹
    """
    if state is None:
        state = ComparisonState(
            document_a=[],
            document_b=[],
            current_chunk_a=None,
            comparison_results=[],
            processed_chunks_a=[],
            agent_thoughts=[],
            current_step="initialize"
        )
    
    state["current_step"] = "select_next_chunk"
    return state

# æ¬¡ã®ãƒãƒ£ãƒ³ã‚¯é¸æŠé–¢æ•°
def select_next_chunk_from_doc_a(state: ComparisonState) -> Tuple[ComparisonState, str]:
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆAã‹ã‚‰æ¬¡ã®å‡¦ç†å¯¾è±¡ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠã™ã‚‹
    
    Args:
        state: ç¾åœ¨ã®æ¯”è¼ƒçŠ¶æ…‹
        
    Returns:
        æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹ã¨æ¬¡ã®ãƒãƒ¼ãƒ‰å
    """
    # å‡¦ç†æ¸ˆã¿ã§ãªã„ãƒãƒ£ãƒ³ã‚¯ã‚’æ¢ã™
    for chunk in state["document_a"]:
        if chunk["id"] not in state["processed_chunks_a"]:
            state["current_chunk_a"] = chunk
            state["current_step"] = "react_agent"
            return state, "has_more_chunks"
    
    # ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯ãŒå‡¦ç†æ¸ˆã¿ã®å ´åˆ
    state["current_chunk_a"] = None
    state["current_step"] = "format_output"
    return state, "no_more_chunks"

# æ¯”è¼ƒçµæœã®å‡¦ç†é–¢æ•°
def process_comparison_results(state: ComparisonState) -> ComparisonState:
    """
    æ¯”è¼ƒçµæœã‚’å‡¦ç†ã™ã‚‹
    
    Args:
        state: ç¾åœ¨ã®æ¯”è¼ƒçŠ¶æ…‹
        
    Returns:
        æ›´æ–°ã•ã‚ŒãŸæ¯”è¼ƒçŠ¶æ…‹
    """
    # å¿…è¦ã«å¿œã˜ã¦çµæœã®å¾Œå‡¦ç†ã‚’è¡Œã†
    state["current_step"] = "select_next_chunk"
    return state

# å‡ºåŠ›æ•´å½¢é–¢æ•°
def format_output_as_markdown_table(state: ComparisonState) -> ComparisonState:
    """
    æ¯”è¼ƒçµæœã‚’è¡¨å½¢å¼ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã«æ•´å½¢ã™ã‚‹
    
    Args:
        state: ç¾åœ¨ã®æ¯”è¼ƒçŠ¶æ…‹
        
    Returns:
        æ›´æ–°ã•ã‚ŒãŸæ¯”è¼ƒçŠ¶æ…‹ï¼ˆå‡ºåŠ›çµæœã‚’å«ã‚€ï¼‰
    """
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ˜ãƒƒãƒ€ãƒ¼
    markdown = "# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¯”è¼ƒçµæœ\n\n"
    markdown += "## æ¦‚è¦\n\n"
    
    # çµ±è¨ˆæƒ…å ±
    total = len(state["comparison_results"])
    changes = sum(1 for r in state["comparison_results"] if r["type"] == "å¤‰æ›´")
    additions = sum(1 for r in state["comparison_results"] if r["type"] == "è¿½åŠ ")
    deletions = sum(1 for r in state["comparison_results"] if r["type"] == "å‰Šé™¤")
    matches = sum(1 for r in state["comparison_results"] if r["type"] == "ä¸€è‡´")
    
    markdown += f"- ç·ãƒãƒ£ãƒ³ã‚¯æ•°: {total}\n"
    markdown += f"- å¤‰æ›´: {changes}\n"
    markdown += f"- è¿½åŠ : {additions}\n"
    markdown += f"- å‰Šé™¤: {deletions}\n"
    markdown += f"- ä¸€è‡´: {matches}\n\n"
    
    # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«
    markdown += "## è©³ç´°æ¯”è¼ƒçµæœ\n\n"
    markdown += "| No. | å¤‰æ›´ã‚¿ã‚¤ãƒ— | ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆA | ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆB | è©³ç´° |\n"
    markdown += "|-----|------------|--------------|--------------|------|\n"
    
    for i, result in enumerate(state["comparison_results"], 1):
        # å¤‰æ›´ã‚¿ã‚¤ãƒ—ã®ã‚¢ã‚¤ã‚³ãƒ³
        type_icon = {
            "å¤‰æ›´": "ğŸ”„ å¤‰æ›´",
            "è¿½åŠ ": "â• è¿½åŠ ",
            "å‰Šé™¤": "âŒ å‰Šé™¤",
            "ä¸€è‡´": "âœ“ ä¸€è‡´"
        }.get(result["type"], result["type"])
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆAã®å†…å®¹
        doc_a_content = result.get("chunk_a", {}).get("content", "-") if result["type"] != "è¿½åŠ " else "-"
        if len(doc_a_content) > 100:
            doc_a_content = doc_a_content[:97] + "..."
            
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆBã®å†…å®¹
        doc_b_content = result.get("chunk_b", {}).get("content", "-") if result["type"] != "å‰Šé™¤" else "-"
        if len(doc_b_content) > 100:
            doc_b_content = doc_b_content[:97] + "..."
        
        # è©³ç´°æƒ…å ±
        details = result.get("details", "")
        
        # è¡Œã®è¿½åŠ 
        markdown += f"| {i} | {type_icon} | {doc_a_content} | {doc_b_content} | {details} |\n"
    
    # çµæœã‚’çŠ¶æ…‹ã«ä¿å­˜
    state["markdown_output"] = markdown
    state["current_step"] = "complete"
    
    return state

# Langgraphã®æ§‹ç¯‰
def build_document_comparison_graph():
    """
    ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¯”è¼ƒç”¨ã®Langgraphã‚’æ§‹ç¯‰ã™ã‚‹
    
    Returns:
        æ§‹ç¯‰ã•ã‚ŒãŸStateGraph
    """
    # ã‚°ãƒ©ãƒ•ã®å®šç¾©
    workflow = StateGraph(ComparisonState)
    
    # ãƒãƒ¼ãƒ‰ã®è¿½åŠ 
    workflow.add_node("initialize", initialize_comparison)
    workflow.add_node("select_next_chunk", select_next_chunk_from_doc_a)
    workflow.add_node("react_agent", react_agent)
    workflow.add_node("process_comparison_results", process_comparison_results)
    workflow.add_node("format_output", format_output_as_markdown_table)
    
    # ã‚¨ãƒƒã‚¸ã®è¿½åŠ 
    workflow.add_edge("initialize", "select_next_chunk")
    workflow.add_conditional_edges(
        "select_next_chunk",
        lambda state, result: result,
        {
            "has_more_chunks": "react_agent",
            "no_more_chunks": "format_output"
        }
    )
    workflow.add_edge("react_agent", "process_comparison_results")
    workflow.add_edge("process_comparison_results", "select_next_chunk")
    workflow.add_edge("format_output", END)
    
    # ã‚°ãƒ©ãƒ•ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    return workflow.compile()

# ãƒ¡ã‚¤ãƒ³é–¢æ•°
def compare_documents(document_a: List[Dict], document_b: List[Dict]) -> str:
    """
    2ã¤ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¯”è¼ƒã™ã‚‹
    
    Args:
        document_a: æ¯”è¼ƒå…ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆï¼‰
        document_b: æ¯”è¼ƒå…ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆãƒãƒ£ãƒ³ã‚¯ãƒªã‚¹ãƒˆï¼‰
        
    Returns:
        æ¯”è¼ƒçµæœã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³æ–‡å­—åˆ—
    """
    # ã‚°ãƒ©ãƒ•ã®æ§‹ç¯‰
    graph = build_document_comparison_graph()
    
    # åˆæœŸçŠ¶æ…‹ã®è¨­å®š
    initial_state = ComparisonState(
        document_a=document_a,
        document_b=document_b,
        current_chunk_a=None,
        comparison_results=[],
        processed_chunks_a=[],
        agent_thoughts=[],
        current_step="initialize"
    )
    
    # ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œ
    result = graph.invoke(initial_state)
    
    # çµæœã®å–å¾—
    return result["markdown_output"]

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
    document_a = [
        {"id": "A1", "content": "ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨ˆç”»æ›¸ã®ãƒ‰ãƒ©ãƒ•ãƒˆã§ã™ã€‚"},
        {"id": "A2", "content": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåï¼šã‚·ã‚¹ãƒ†ãƒ åˆ·æ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ"},
        {"id": "A3", "content": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®æ¨™ã¯ã€é¡§å®¢æº€è¶³åº¦ã‚’20%å‘ä¸Šã•ã›ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®3ã¤ã®æ–½ç­–ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚"},
        {"id": "A4", "content": "äºˆç®—ã¯500ä¸‡å††ã‚’ä¸Šé™ã¨ã—ã¾ã™ã€‚"}
    ]
    
    document_b = [
        {"id": "B1", "content": "ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨ˆç”»æ›¸ã®ãƒ‰ãƒ©ãƒ•ãƒˆã§ã™ã€‚"},
        {"id": "B2", "content": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåï¼šã‚·ã‚¹ãƒ†ãƒ åˆ·æ–°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ2023"},
        {"id": "B3", "content": "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®æ¨™ã¯ã€é¡§å®¢æº€è¶³åº¦ã‚’25%å‘ä¸Šã•ã›ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã‚’é”æˆã™ã‚‹ãŸã‚ã«ã€ä»¥ä¸‹ã®4ã¤ã®æ–½ç­–ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚"},
        {"id": "B4", "content": "æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€çµŒå–¶é™£ã®æ‰¿èªã‚’å¾—ã¦2023å¹´4æœˆ1æ—¥ã«é–‹å§‹ã•ã‚Œã¾ã—ãŸã€‚"}
    ]
    
    # æ¯”è¼ƒã®å®Ÿè¡Œ
    result = compare_documents(document_a, document_b)
    
    # çµæœã®å‡ºåŠ›
    print(result)
```

## ä½¿ç”¨æ–¹æ³•

1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼š

```bash
pip install langchain langchain-openai langgraph numpy
```

2. OpenAI APIã‚­ãƒ¼ã‚’è¨­å®šã—ã¾ã™ï¼š

```python
os.environ["OPENAI_API_KEY"] = "your-api-key-here"
```

3. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯åŒ–ã—ã¦æ¯”è¼ƒã‚’å®Ÿè¡Œã—ã¾ã™ï¼š

```python
# æ—¢ã«ãƒãƒ£ãƒ³ã‚¯åŒ–ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æº–å‚™
document_a = [
    {"id": "A1", "content": "ãƒãƒ£ãƒ³ã‚¯1ã®å†…å®¹"},
    {"id": "A2", "content": "ãƒãƒ£ãƒ³ã‚¯2ã®å†…å®¹"},
    # ...
]

document_b = [
    {"id": "B1", "content": "ãƒãƒ£ãƒ³ã‚¯1ã®å†…å®¹"},
    {"id": "B2", "content": "ãƒãƒ£ãƒ³ã‚¯2ã®å†…å®¹"},
    # ...
]

# æ¯”è¼ƒã®å®Ÿè¡Œ
result = compare_documents(document_a, document_b)

# çµæœã®ä¿å­˜
with open("comparison_result.md", "w", encoding="utf-8") as f:
    f.write(result)
```

4. æ¯”è¼ƒçµæœã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ“ãƒ¥ãƒ¼ã‚¢ã§ç¢ºèªã—ã¾ã™ã€‚
